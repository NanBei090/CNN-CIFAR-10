digraph {
	graph [size="55.65,55.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1652284545120 [label="
 (128, 10)" fillcolor=darkolivegreen1]
	1652284144992 [label=AddmmBackward0]
	1652284144656 -> 1652284144992
	1652284444816 [label="fc.bias
 (10)" fillcolor=lightblue]
	1652284444816 -> 1652284144656
	1652284144656 [label=AccumulateGrad]
	1652284143552 -> 1652284144992
	1652284143552 [label=ViewBackward0]
	1652284145376 -> 1652284143552
	1652284145376 [label=MeanBackward1]
	1652284144752 -> 1652284145376
	1652284144752 [label=ReluBackward0]
	1652284145136 -> 1652284144752
	1652284145136 [label=AddBackward0]
	1652284144080 -> 1652284145136
	1652284144080 [label=NativeBatchNormBackward0]
	1652284143888 -> 1652284144080
	1652284143888 [label=ConvolutionBackward0]
	1652284143648 -> 1652284143888
	1652284143648 [label=ReluBackward0]
	1652283960640 -> 1652284143648
	1652283960640 [label=NativeBatchNormBackward0]
	1652284522848 -> 1652283960640
	1652284522848 [label=ConvolutionBackward0]
	1652284144032 -> 1652284522848
	1652284144032 [label=ReluBackward0]
	1652284523664 -> 1652284144032
	1652284523664 [label=AddBackward0]
	1652284524000 -> 1652284523664
	1652284524000 [label=NativeBatchNormBackward0]
	1652284524480 -> 1652284524000
	1652284524480 [label=ConvolutionBackward0]
	1652284525056 -> 1652284524480
	1652284525056 [label=ReluBackward0]
	1652284525536 -> 1652284525056
	1652284525536 [label=NativeBatchNormBackward0]
	1652284525824 -> 1652284525536
	1652284525824 [label=ConvolutionBackward0]
	1652284526400 -> 1652284525824
	1652284526400 [label=MaxPool2DWithIndicesBackward0]
	1652284526880 -> 1652284526400
	1652284526880 [label=ReluBackward0]
	1652284527216 -> 1652284526880
	1652284527216 [label=AddBackward0]
	1652284527552 -> 1652284527216
	1652284527552 [label=NativeBatchNormBackward0]
	1652284527936 -> 1652284527552
	1652284527936 [label=ConvolutionBackward0]
	1652284528560 -> 1652284527936
	1652284528560 [label=ReluBackward0]
	1652284522704 -> 1652284528560
	1652284522704 [label=NativeBatchNormBackward0]
	1652284528608 -> 1652284522704
	1652284528608 [label=ConvolutionBackward0]
	1652284527696 -> 1652284528608
	1652284527696 [label=ReluBackward0]
	1652284528896 -> 1652284527696
	1652284528896 [label=AddBackward0]
	1652284528992 -> 1652284528896
	1652284528992 [label=NativeBatchNormBackward0]
	1652284529136 -> 1652284528992
	1652284529136 [label=ConvolutionBackward0]
	1652284529328 -> 1652284529136
	1652284529328 [label=ReluBackward0]
	1652284529472 -> 1652284529328
	1652284529472 [label=NativeBatchNormBackward0]
	1652284529568 -> 1652284529472
	1652284529568 [label=ConvolutionBackward0]
	1652284529760 -> 1652284529568
	1652284529760 [label=MaxPool2DWithIndicesBackward0]
	1652284529904 -> 1652284529760
	1652284529904 [label=ReluBackward0]
	1652284530000 -> 1652284529904
	1652284530000 [label=AddBackward0]
	1652284530096 -> 1652284530000
	1652284530096 [label=NativeBatchNormBackward0]
	1652284530240 -> 1652284530096
	1652284530240 [label=ConvolutionBackward0]
	1652284530432 -> 1652284530240
	1652284530432 [label=ReluBackward0]
	1652284530576 -> 1652284530432
	1652284530576 [label=NativeBatchNormBackward0]
	1652284530672 -> 1652284530576
	1652284530672 [label=ConvolutionBackward0]
	1652284530144 -> 1652284530672
	1652284530144 [label=ReluBackward0]
	1652284530960 -> 1652284530144
	1652284530960 [label=AddBackward0]
	1652284531056 -> 1652284530960
	1652284531056 [label=NativeBatchNormBackward0]
	1652284531200 -> 1652284531056
	1652284531200 [label=ConvolutionBackward0]
	1652284531392 -> 1652284531200
	1652284531392 [label=ReluBackward0]
	1652284531536 -> 1652284531392
	1652284531536 [label=NativeBatchNormBackward0]
	1652284531632 -> 1652284531536
	1652284531632 [label=ConvolutionBackward0]
	1652284531824 -> 1652284531632
	1652284531824 [label=MaxPool2DWithIndicesBackward0]
	1652284531968 -> 1652284531824
	1652284531968 [label=ReluBackward0]
	1652284532064 -> 1652284531968
	1652284532064 [label=AddBackward0]
	1652284532160 -> 1652284532064
	1652284532160 [label=NativeBatchNormBackward0]
	1652284532304 -> 1652284532160
	1652284532304 [label=ConvolutionBackward0]
	1652284532496 -> 1652284532304
	1652284532496 [label=ReluBackward0]
	1652284532640 -> 1652284532496
	1652284532640 [label=NativeBatchNormBackward0]
	1652284532736 -> 1652284532640
	1652284532736 [label=ConvolutionBackward0]
	1652284532208 -> 1652284532736
	1652284532208 [label=ReluBackward0]
	1652284533024 -> 1652284532208
	1652284533024 [label=AddBackward0]
	1652284533120 -> 1652284533024
	1652284533120 [label=NativeBatchNormBackward0]
	1652284533264 -> 1652284533120
	1652284533264 [label=ConvolutionBackward0]
	1652284533456 -> 1652284533264
	1652284533456 [label=ReluBackward0]
	1652284533600 -> 1652284533456
	1652284533600 [label=NativeBatchNormBackward0]
	1652284533696 -> 1652284533600
	1652284533696 [label=ConvolutionBackward0]
	1652284533168 -> 1652284533696
	1652284533168 [label=ReluBackward0]
	1652284533984 -> 1652284533168
	1652284533984 [label=NativeBatchNormBackward0]
	1652284534080 -> 1652284533984
	1652284534080 [label=ConvolutionBackward0]
	1652284534272 -> 1652284534080
	1652284253248 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	1652284253248 -> 1652284534272
	1652284534272 [label=AccumulateGrad]
	1652284534128 -> 1652284533984
	1652284253328 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1652284253328 -> 1652284534128
	1652284534128 [label=AccumulateGrad]
	1652284533840 -> 1652284533984
	1652284253408 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1652284253408 -> 1652284533840
	1652284533840 [label=AccumulateGrad]
	1652284533888 -> 1652284533696
	1652284253888 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1652284253888 -> 1652284533888
	1652284533888 [label=AccumulateGrad]
	1652284533744 -> 1652284533600
	1652284253968 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1652284253968 -> 1652284533744
	1652284533744 [label=AccumulateGrad]
	1652284533552 -> 1652284533600
	1652284254048 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1652284254048 -> 1652284533552
	1652284533552 [label=AccumulateGrad]
	1652284533504 -> 1652284533264
	1652284254528 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1652284254528 -> 1652284533504
	1652284533504 [label=AccumulateGrad]
	1652284533312 -> 1652284533120
	1652284254608 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1652284254608 -> 1652284533312
	1652284533312 [label=AccumulateGrad]
	1652284533216 -> 1652284533120
	1652284254688 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1652284254688 -> 1652284533216
	1652284533216 [label=AccumulateGrad]
	1652284533168 -> 1652284533024
	1652284532928 -> 1652284532736
	1652284255168 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1652284255168 -> 1652284532928
	1652284532928 [label=AccumulateGrad]
	1652284532784 -> 1652284532640
	1652284255248 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1652284255248 -> 1652284532784
	1652284532784 [label=AccumulateGrad]
	1652284532592 -> 1652284532640
	1652284255328 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1652284255328 -> 1652284532592
	1652284532592 [label=AccumulateGrad]
	1652284532544 -> 1652284532304
	1652284255808 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1652284255808 -> 1652284532544
	1652284532544 [label=AccumulateGrad]
	1652284532352 -> 1652284532160
	1652284255888 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1652284255888 -> 1652284532352
	1652284532352 [label=AccumulateGrad]
	1652284532256 -> 1652284532160
	1652284255968 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1652284255968 -> 1652284532256
	1652284532256 [label=AccumulateGrad]
	1652284532208 -> 1652284532064
	1652284531872 -> 1652284531632
	1652284256688 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1652284256688 -> 1652284531872
	1652284531872 [label=AccumulateGrad]
	1652284531680 -> 1652284531536
	1652284256768 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1652284256768 -> 1652284531680
	1652284531680 [label=AccumulateGrad]
	1652284531488 -> 1652284531536
	1652284256848 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1652284256848 -> 1652284531488
	1652284531488 [label=AccumulateGrad]
	1652284531440 -> 1652284531200
	1652284257328 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1652284257328 -> 1652284531440
	1652284531440 [label=AccumulateGrad]
	1652284531248 -> 1652284531056
	1652284257408 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1652284257408 -> 1652284531248
	1652284531248 [label=AccumulateGrad]
	1652284531152 -> 1652284531056
	1652284257488 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1652284257488 -> 1652284531152
	1652284531152 [label=AccumulateGrad]
	1652284531104 -> 1652284530960
	1652284531104 [label=ConvolutionBackward0]
	1652284531824 -> 1652284531104
	1652284531584 -> 1652284531104
	1652284256448 [label="layer2.0.downsample.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1652284256448 -> 1652284531584
	1652284531584 [label=AccumulateGrad]
	1652284530864 -> 1652284530672
	1652284257888 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1652284257888 -> 1652284530864
	1652284530864 [label=AccumulateGrad]
	1652284530720 -> 1652284530576
	1652284257968 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1652284257968 -> 1652284530720
	1652284530720 [label=AccumulateGrad]
	1652284530528 -> 1652284530576
	1652284258048 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1652284258048 -> 1652284530528
	1652284530528 [label=AccumulateGrad]
	1652284530480 -> 1652284530240
	1652284258528 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1652284258528 -> 1652284530480
	1652284530480 [label=AccumulateGrad]
	1652284530288 -> 1652284530096
	1652284258608 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1652284258608 -> 1652284530288
	1652284530288 [label=AccumulateGrad]
	1652284530192 -> 1652284530096
	1652284258688 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1652284258688 -> 1652284530192
	1652284530192 [label=AccumulateGrad]
	1652284530144 -> 1652284530000
	1652284529808 -> 1652284529568
	1652284259408 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1652284259408 -> 1652284529808
	1652284529808 [label=AccumulateGrad]
	1652284529616 -> 1652284529472
	1652284259488 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1652284259488 -> 1652284529616
	1652284529616 [label=AccumulateGrad]
	1652284529424 -> 1652284529472
	1652284259568 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1652284259568 -> 1652284529424
	1652284529424 [label=AccumulateGrad]
	1652284529376 -> 1652284529136
	1652284260048 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1652284260048 -> 1652284529376
	1652284529376 [label=AccumulateGrad]
	1652284529184 -> 1652284528992
	1652284260128 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1652284260128 -> 1652284529184
	1652284529184 [label=AccumulateGrad]
	1652284529088 -> 1652284528992
	1652284260208 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1652284260208 -> 1652284529088
	1652284529088 [label=AccumulateGrad]
	1652284529040 -> 1652284528896
	1652284529040 [label=ConvolutionBackward0]
	1652284529760 -> 1652284529040
	1652284529520 -> 1652284529040
	1652284259168 [label="layer3.0.downsample.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1652284259168 -> 1652284529520
	1652284529520 [label=AccumulateGrad]
	1652284528800 -> 1652284528608
	1652284440896 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1652284440896 -> 1652284528800
	1652284528800 [label=AccumulateGrad]
	1652284528656 -> 1652284522704
	1652284440976 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1652284440976 -> 1652284528656
	1652284528656 [label=AccumulateGrad]
	1652284528464 -> 1652284522704
	1652284441056 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1652284441056 -> 1652284528464
	1652284528464 [label=AccumulateGrad]
	1652284528512 -> 1652284527936
	1652284441536 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1652284441536 -> 1652284528512
	1652284528512 [label=AccumulateGrad]
	1652284528128 -> 1652284527552
	1652284441616 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1652284441616 -> 1652284528128
	1652284528128 [label=AccumulateGrad]
	1652284527792 -> 1652284527552
	1652284441696 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1652284441696 -> 1652284527792
	1652284527792 [label=AccumulateGrad]
	1652284527696 -> 1652284527216
	1652284526592 -> 1652284525824
	1652284442416 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1652284442416 -> 1652284526592
	1652284526592 [label=AccumulateGrad]
	1652284526016 -> 1652284525536
	1652284442496 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1652284442496 -> 1652284526016
	1652284526016 [label=AccumulateGrad]
	1652284525344 -> 1652284525536
	1652284442576 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1652284442576 -> 1652284525344
	1652284525344 [label=AccumulateGrad]
	1652284525200 -> 1652284524480
	1652284442976 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1652284442976 -> 1652284525200
	1652284525200 [label=AccumulateGrad]
	1652284524624 -> 1652284524000
	1652284443056 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1652284443056 -> 1652284524624
	1652284524624 [label=AccumulateGrad]
	1652284524288 -> 1652284524000
	1652284443136 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1652284443136 -> 1652284524288
	1652284524288 [label=AccumulateGrad]
	1652284524144 -> 1652284523664
	1652284524144 [label=ConvolutionBackward0]
	1652284526400 -> 1652284524144
	1652284525680 -> 1652284524144
	1652284442176 [label="layer4.0.downsample.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1652284442176 -> 1652284525680
	1652284525680 [label=AccumulateGrad]
	1652284523328 -> 1652284522848
	1652284443536 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1652284443536 -> 1652284523328
	1652284523328 [label=AccumulateGrad]
	1652284522992 -> 1652283960640
	1652284443616 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1652284443616 -> 1652284522992
	1652284522992 [label=AccumulateGrad]
	1652284522608 -> 1652283960640
	1652284443696 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1652284443696 -> 1652284522608
	1652284522608 [label=AccumulateGrad]
	1652282375184 -> 1652284143888
	1652284444176 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1652284444176 -> 1652282375184
	1652282375184 [label=AccumulateGrad]
	1652284143840 -> 1652284144080
	1652284444256 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1652284444256 -> 1652284143840
	1652284143840 [label=AccumulateGrad]
	1652284143984 -> 1652284144080
	1652284444336 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1652284444336 -> 1652284143984
	1652284143984 [label=AccumulateGrad]
	1652284144032 -> 1652284145136
	1652284144896 -> 1652284144992
	1652284144896 [label=TBackward0]
	1652284139280 -> 1652284144896
	1652284444736 [label="fc.weight
 (10, 512)" fillcolor=lightblue]
	1652284444736 -> 1652284139280
	1652284139280 [label=AccumulateGrad]
	1652284144992 -> 1652284545120
}
